# 【BERT 系列 2】之 XLNet

### 本着互相尊重的原则，如需转载请附加原文链接，非常感谢！


#### 本文收获
* 自回归（AutoRegressive, AR）和自编码（AutoEncoder, AE）思想的区别？
* XLNet 的研究出发点是什么？
* 什么是双流注意力机制（Two-steam Attention）？
* XLNet 和 BERT 的对比。

#### 重要文章
* <span id = "paper1">Paper 1</span>:[XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237.pdf)
* <span id = "paper2">Paper 2</span>:[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
---
**本文收获**和**重要文章**我先列在前面，以使得在读正文之前能有个概念。文章也会根据**本文收获**的逻辑路线来写。

之所以把 XLNet 归到 BERT 系列是因为 XLNet 的逻辑是提升 BERT 模型天然的短板，弥补了 BERT 中的两个缺陷。加上在他们之后发布的文章很多也会拿他们俩来比较，我在这里也把他们分到同一个系列中。原文章里涉及到比较多的公式，我这里能省就省了。对 BERT 不了解的童鞋们，可以先读一下 [#【BERT 系列 1】之 BERT 本尊](https://zhuanlan.zhihu.com/p/94513051)

### 1. 自回归（AutoRegressive, AR）和自编码（AutoEncoder, AE）
**自回归**，如果了解语言模型的话，自回归就很好理解了。拿预测一段文字序列来举例子，AR 的意思是依赖于前面的文字来生成后面的文字。那下面的例子来说，假设给一段文字 ：

> 完整文字段：知乎问答不回答专栏的文章很好很棒很赞
> 部分文字段：知乎问答不回答专栏的文章
> 需预测文字段：很好很棒很赞

现在的要求是给定 `部分分字段` 来预测 `需预测文字段`，那么就要根据 `知乎问答不回答专栏的文章` 先来预测 `很`，再根据 `知乎问答不回答专栏的文章很` 来预测 `好`。接下来依次类推，每次把预测出来的文字加入到 `部分文字段` 中，来预测下一个字。这是基本语言模型的原理。

**自编码**，自编码则和自回归不一样，他是在训练集中引入噪音，再来预测自己本身。他的思路是，在有噪音的训练集中把自己成功的还原，则这个模型就有去噪的能力，那么他表征自己的能力也就更强了。还是拿上面的例子来解释。

> 完整文字段：知乎问答不回答专栏的文章很好很棒很赞
> 部分文字段：知乎问答 [MASK] 回答专栏的 [MASK] 很好 [MASK] 很赞
> 需预测文字段：不  文章  很棒

可以看到，他是把原文字段中的某些字或词 mask 掉（通过占位符 [MASK]来替代原有文字），这就相当于去掉了部分有用信息而引入了部分噪音。其预测的目标是把这些去掉的信息还原回来。

### 2. XLNet 的出发点
正如上面介绍的，语言模型（Language Model，LM）属于自回归方式来训练，BERT 则正是属于自编码的训练方式。我们首先来说 BERT，基于上面的分析，BERT 存在两个问题。
**BERT 问题1：预测独立性假设**，在预测 [MASK] 位置的词的时候，每个 [MASK] 都是独立地预测，比如：

> 原句子：深圳是个大都市
> Mask 句子：[MASK] [MASK] 是个大都市

可以看到，在预测的时候，这两个 [MASK] 相当于是这样预测的，在给定 `是个大都市` 的条件下，预测 `深` 和 `圳` 的概率分别是：p(深 | 是个大都市) 和 p(圳 | 是个大都市)。而这显然是有缺陷的，没有考虑到 `深圳` 这两个词的关系（当然了，这里不考虑汉语的词表，只考虑单个字）。
**BERT 问题2：训练-预测不一致**，在训练好 BERT 与训练模型以后，在下游任务进行 fine-tuning。但是下游任务是没有 [MASK] 标记的。这就引起了训练-预测不一致的问题，虽然说在 BERT 训练好了以后，基本上能够生成出每个词的语义向量，但是毕竟有 [MASK] 的存在，这不是一个自然的预测下游任务的方式。

**语言模型问题：单向模型**，说完了 BERT，我们再来说语言模型，虽然语言模型没有上述问题，但是其最大的短板是非双向可达，双向可达是指在计算一个字的语义的时候，这个语义应该既考虑到前面的字的信息，又考虑到后面的字的信息。BERT 已经明确验证过双向语义模型对于语义表达的重要性。但是在语言模型中，只有单向，最常见的是从左向右建模，即右边的字只能看到其左边的字的信息。语言模型的优点是符合自然预测的习惯即根据前面的信息，来生成后面的文字。

BERT 和语言模型有各自的优缺点，那么有没有一种方式能结合两者的优点呢？即结合 BERT 的双向语义建模和语言模型从左至右预测的特点。当然有啦，就是 XLNet 咯。

### XLNet 之排列语言模型（Permutation Language Model）
为了解决上面的问题，XLNet 提出排列语言模型。




---
> [“知乎专栏-问答不回答”](https://zhuanlan.zhihu.com/question-no-answer)，一个期待问答能回答的专栏。
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTY4Nzg2ODU4MywtMTY5NTEwOTc0MCwtMT
AzODE4OTI2OCwtOTU5OTEyNDgsLTg0NDA3MzUyLDMwNjcwMjg3
OSwtMTM4MzkyMTM5MSwtNTUzODgwODM1LC0xNzA4ODQ1Nzg2XX
0=
-->
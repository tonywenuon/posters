# 【BERT 系列 2】之 XLNet

### 本着互相尊重的原则，如需转载请附加原文链接，非常感谢！


#### 本文收获
* 什么是 BERT？BERT 的创作动机是什么？
* BERT 最重要的贡献是什么？
* 如何用 BERT 来实现几个下游 NLP 任务？
* BERT 用到了哪些数据集？以及数据集的简介和地址是什么？
* 如何实现 BERT 代码？

#### 重要文章
* <span id = "paper1">Paper 1</span>:[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
---
**本文收获**和**重要文章**我先列在前面，以使得在读正文之前能有个概念。文章也会根据**本文收获**的逻辑路线来写。


---
> [“知乎专栏-问答不回答”](https://zhuanlan.zhihu.com/question-no-answer)，一个期待问答能回答的专栏。
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE3MDg4NDU3ODZdfQ==
-->